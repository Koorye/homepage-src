---
title: 【机器学习自学笔记1】信息熵
date: 2020-10-28 18:04:32
tags: [机器学习,概率论]
categories: 机器学习
description: 信息熵是一种衡量信息不确定性的物理量，信息熵越大，信息越难以确定。信息熵可以衡量事件的不确定性，熵越大，事件就越不确定。
---

一直想记录一下机器学习的笔记，奈何水平不足，不过还是想挑战一下自己。话不多说，下面开始。

# 熵

> **熵 **(entropy) 泛指某些物质系统状态的一种量度，某些物质系统状态可能出现的程度。亦被社会科学用以借喻人类社会某些状态的程度。



## 信息熵

### 信息熵的由来

> 信息熵这个词是C.E.Shannon（香农）从热力学中借用过来的。热力学中的热熵是表示分子状态混乱程度的物理量。香农用信息熵的概念来描述信源的不确定度。

信息熵是一种衡量信息不确定性的物理量，信息熵越大，信息越难以确定。

一个例子：抛一枚硬币，其为正的概率为 1/2，为负的概率为 1/2.

则此时抛硬币的结果有 2 种情况。

若抛 2 枚硬币，则有 (正, 正), (正, 负), (负, 正), (负, 负) 4 种情况。

...

若抛 n 枚硬币，则有 $2^n $种情况。

注意到硬币的个数 (n) 与情况数 (m) 的关系:
$$
n = \log_2m
$$
这就是信息熵的计算公式。

对于事件 X，其拥有 m 个等概率的不确定情况，其信息熵为(单位：bit):
$$
Ent(X) = \log_2m
$$
即
$$
Ent(X) = -\log_2 \frac{1}{m} = -\log_2 p(x)
$$


其中 p(x) 即表示对于每一种情况，其发生的概率。



### 概率不相等情况的信息熵

以上是等概率条件下的信息熵，但如果概率不相等，要怎么计算呢？

举一个例子：

| 情况 | A    | B    | C    |
| ---- | ---- | ---- | ---- |
| p    | 1/2  | 1/3  | 1/6  |

我们可以分别计算出每种情况的信息熵：
$$
Ent(X|A) = -\log_2 \frac{1}{2} = 1
$$

$$
Ent(X|B) = -log_2\frac{1}{3} = \log_2 3
$$

$$
Ent(X|C) = -log_2\frac{1}{6} = \log_2 6
$$

再根据每种情况的权重进行累加，得到总体的信息熵：
$$
Ent(X) = P(A)Ent(X|A) + P(B)Ent(X|B) + P(C)Ent(X|C) = \frac{1}{2}+\frac{1}{3}\log_2 3+\frac{1}{6}\log_2 6 = 1.4591
$$
于是我们有信息熵的计算公式。

对于事件 X，情况数为 N ，其每一种情况 xi 的概率为 p(xi)，其信息熵为：
$$
Ent(X) = \sum_{i=1}^{N} p(x_i)Ent(X|x_i) = -\sum_{i=1}^{N}p(x_i)\log_2 p(x_i)
$$
*注：一般我们用 2 作为信息熵的底数，而在 sklearn 中，其底数则默认为 e.*

信息熵可以衡量事件的不确定性，熵越大，事件就越不确定，如抛硬币：

如果其为正、负的概率各为1/2，此时要猜中正负显然是困难的，其信息熵为：
$$
Ent = - \frac{1}{2}log_2 \frac{1}{2} - \frac{1}{2}log_2 \frac{1}{2} = 1
$$
但如果为正、负的概率分别为 9/10, 1/10 呢？

此时猜中正负显然简单许多，其信息熵为：
$$
Ent = - \frac{9}{10}\log_2\frac{9}{10} - \frac{1}{10}\log_2\frac{1}{10} = 0.4690
$$
第二种情况的熵小于第一种情况，第二种情况的结果显然更容易确定。



## 交叉熵

> 交叉熵（Cross Entropy）是Shannon信息论中一个重要概念，主要用于度量两个概率分布间的差异性信息。

交叉熵是一种衡量两个概率分布间差异的度量，它在机器学习领域有着广泛的应用，比如对于真实结果 Y 与训练集得出的结果 $\hat{Y}$，二者之间的交叉熵：
$$
Ent(Y-\hat{Y})
$$
是一种衡量训练准确程度的标志。

对于 P,Q 两种概率分布，交叉熵的计算公式为：
$$
Ent(P,Q) = - \sum_i p(i)\log_2 q(i)
$$
举一个例子，如果有一个训练集的真实结果 P = {1/4, 1/4, 1/4, 1/4}

而训练得到的预测结果 Q = {1/4, 1/8, 1/8, 1/2}

则其交叉熵为：
$$
Ent(P,Q) = -\frac{1}{4}\log_2\frac{1}{4}-\frac{1}{4}\log_2\frac{1}{8}-\frac{1}{4}\log_2\frac{1}{8}-\frac{1}{4}\log_2\frac{1}{2} = 2.25
$$
如果预测结果与真实结果完全相符呢？

即 Q = P = {1/4, 1/4, 1/4, 1/4}
$$
Ent(P,Q) = -\frac{1}{4}\log_2\frac{1}{4}-\frac{1}{4}\log_2\frac{1}{4}-\frac{1}{4}\log_2\frac{1}{4}-\frac{1}{4}\log_2\frac{1}{4} = 2
$$
注意到当两个概率分布相同时，交叉熵 = 信息熵。

同时得出另一个结论，交叉熵越小，两个概率分布就越相似。



## 相对熵

既然交叉熵可以衡量两个概率分布的相似程度，其最小值又等于信息熵，那为什么不令一个值 = 交叉熵 - 信息熵，其为 0 则表示两个概率分布完全相同呢？

这就是相对熵，又称 KL(Kullback-Leibler) 散度。

相对熵的计算公式为：
$$
KL(P||Q) = \sum_i p(i)\log_2\frac{p(i)}{q(i)}
$$
以上面的例子，P = {1/4, 1/4, 1/4, 1/4}, Q = {1/4, 1/8, 1/8, 1/2}，其相对熵为：
$$
KL(P||Q) = \frac{1}{4}\log_2\frac{1/4}{1/4}+\frac{1}{4}\log_2\frac{1/4}{1/8}+\frac{1}{4}\log_2\frac{1/4}{1/8}+\frac{1}{4}\log_2\frac{1/4}{1/2} = 0.25
$$
注意到这个结果即为交叉熵 Ent(P,Q) - 信息熵 Ent(P) 的值。

若 P = Q = {1/4, 1/4, 1/4, 1/4}
$$
KL(P||Q) = \frac{1}{4}\log_2\frac{1/4}{1/4}+\frac{1}{4}\log_2\frac{1/4}{1/4}+\frac{1}{4}\log_2\frac{1/4}{1/4}+\frac{1}{4}\log_2\frac{1/4}{1/4} = 0
$$
当两个概率分布完全相同时，其条件熵 KL(P||Q) = 0.



## 条件熵

条件熵，即在给定条件下的不确定性，衡量一个概率分布对另一个概率分布的期望。
$$
Ent(Y|X) = \sum_x p(x)Ent(Y|X=x)
$$

$$
= - \sum_xp(x)\sum_yp(y|x)\log_2p(y|x)
$$

$$
= - \sum_x\sum_yp(x,y)\log_2p(y|x)
$$

举一个例子：

| 颜色 | 是不是好瓜 |
| ---- | ---------- |
| 青绿 | 是         |
| 青绿 | 否         |
| 青绿 | 是         |
| 深绿 | 是         |
| 深绿 | 否         |

若要求是不是好瓜 Y 关于颜色 X 的条件熵：

首先青绿有 3 个瓜，深绿有 2 个瓜，则 P{X=青绿} = 3/5, P{X=深绿} = 2/5.

则在 X=青绿 中，有 2 个是好瓜，1 个不是好瓜：
$$
Ent(Y|X=青绿) = -\frac{2}{3}\log_2\frac{2}{3}-\frac{1}{3}\log_2\frac{1}{3} = 0.9183
$$
在 X=深绿 中，有 1 个是好瓜，1 个不是好瓜：
$$
Ent(Y|X=深绿) = -\frac{1}{2}\log_2\frac{1}{2}-\frac{1}{2}\log_2\frac{1}{2} = 1
$$
得：
$$
Ent(Y|X) = p(X=青绿)Ent(Y|X=青绿)+p(X=深绿)Ent(Y|X=深绿)
$$

$$
 = \frac{3}{5}\cdot 0.9183 + \frac{2}{5}\cdot 1 = 0.9510
$$



# 增益系数

所谓信息增益系数，即在获取信息前后熵的差值。

举一个例子：

做一道单项选择题，有ABCD四个选项。

此时由于什么都不知道，其每个选项的概率为1/4：
$$
Ent = -\frac{1}{4}\log_2\frac{1}{4} -\frac{1}{4}\log_2\frac{1}{4} -\frac{1}{4}\log_2\frac{1}{4} -\frac{1}{4}\log_2\frac{1}{4} = 2
$$
如果得知一条信息：选 C 的概率是 2/3：

此时 ABCD 的概率分别为 1/9, 1/9, 2/3, 1/9:
$$
Ent = -\frac{1}{9}\log_2\frac{1}{9}-\frac{1}{9}\log_2\frac{1}{9}-\frac{2}{3}\log_2\frac{2}{3}-\frac{1}{9}\log_2\frac{1}{9} = 1.4466
$$
则这条信息的增益系数为：
$$
Gain = 2 - 1.4466 = 0.5534
$$


# 基尼系数

基尼系数是衡量集合纯度的量，基尼系数越低，集合越纯(不确定性越低)；基尼系数越高，集合越不纯(不确定性越高)。

基尼系数的计算公式：
$$
Gini = 1 - \sum_i p(i)^2
$$
比较信息熵的计算公式：
$$
Ent = -\sum_i p(i) \log_2 p(i)
$$
就会发现两者其实区别不大，只不过是把 log 去掉了而已。

对于上面抛硬币的例子：

如果正反的概率各为 1/2：
$$
Gini = 1 - (\frac{1}{2})^2 - (\frac{1}{2})^2 = \frac{1}{2}
$$
如果正反的概率分别为 9/10, 1/10：
$$
Gini = 1 - (\frac{9}{10})^2 - (\frac{1}{10})^2 = 0.18
$$
后者小于前者，说明后者的纯度更高，不确定性更低。
